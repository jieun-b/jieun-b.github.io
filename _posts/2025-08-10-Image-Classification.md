---
title: "이미지 분류(Image Classification) 기초: 개념부터 선형 분류기까지"
date: 2025-08-10 00:00:00 +0900
layout: post
categories: [공부, Deep Learning]
tags: [Computer Vision, Image Classification, KNN, Linear Classifier]
math: true
---

# 1. 이미지 분류란?

이미지 분류(Image Classification)는 **입력 이미지를 미리 정의된 카테고리 중 하나로 분류하는 작업**이다. 자율주행의 신호등 인식, 의료 영상에서의 질병 진단, 제조 라인의 불량품 검출 등 다양한 분야에서 핵심적으로 활용된다.

## 1.1 Semantic Gap

이미지 분류가 어려운 근본적인 이유는 **Semantic Gap** 때문이다. 인간에게 이미지는 "고양이"라는 의미를 가지지만, 컴퓨터에게는 0~255 사이의 정수로 이루어진 3차원 배열에 불과하다.

32×32 픽셀의 RGB 이미지를 예로 들면, 이는 32 × 32 × 3 = **3,072개의 숫자**로 표현된다. 컴퓨터는 이 숫자들의 나열에서 "고양이"라는 고차원적 의미를 추출해야 한다.

문제를 더 어렵게 만드는 것은 **동일한 객체도 조건에 따라 완전히 다른 픽셀 값을 가진다**는 점이다. 시점(Viewpoint), 조명(Illumination), 자세 변형(Deformation), 가림(Occlusion), 배경(Background Clutter), 클래스 내 변형(Intra-class Variation) 등 다양한 요인이 있으며, 이러한 변형에도 불구하고 일관된 분류 결과를 내는 것이 핵심 과제다.

## 1.2 Data-Driven Approach

초기 컴퓨터 비전에서는 "뾰족한 귀 + 수염 → 고양이"와 같은 **규칙 기반 접근**을 시도했다. 그러나 이 방식은 수천 개의 카테고리에 대해 일일이 규칙을 정의해야 하고, 앞서 언급한 다양한 변형을 모두 처리할 수 없다는 한계가 있다.

현대적 접근법은 **데이터 중심(Data-Driven)**이다:

1. 대량의 (이미지, 라벨) 쌍으로 구성된 데이터셋 수집
2. 학습 알고리즘을 통해 데이터에서 패턴 추출
3. 학습된 모델로 새로운 이미지 분류

이 패러다임에서 알고리즘은 명시적 규칙 없이 데이터로부터 분류 기준을 스스로 학습한다.

# 2. K-최근접 이웃 (K-Nearest Neighbors)

K-NN은 가장 직관적인 데이터 중심 분류기로, **"유사한 이미지는 같은 클래스에 속할 것"**이라는 가정에 기반한다.

## 2.1 알고리즘

- **학습 단계**: 별도의 학습 과정 없이 모든 학습 데이터를 메모리에 저장한다.
- **예측 단계**: 새로운 이미지가 입력되면 저장된 모든 학습 이미지와의 거리를 계산하고, 거리가 가장 가까운 K개의 이웃을 선택한 뒤, **다수결 투표**로 최종 클래스를 결정한다.

K=1인 경우(Nearest Neighbor)는 가장 가까운 단일 이웃의 라벨을 그대로 반환한다. 그러나 이는 노이즈나 이상치에 취약하다. 예를 들어, 클래스 A 영역 한가운데 잘못 라벨링된 클래스 B 샘플이 하나 있으면, 그 주변의 모든 테스트 샘플이 B로 잘못 분류된다. K를 늘리면 여러 이웃의 의견을 종합하므로 이런 노이즈의 영향이 줄어들고, 결정 경계가 부드러워진다.

## 2.2 거리 척도 (Distance Metric)

두 이미지 간의 "유사도"를 측정하기 위해 거리 함수를 정의해야 한다.

**L1 Distance (Manhattan)**

$$d_{L1}(x, y) = \sum_i |x_i - y_i|$$

각 픽셀 차이의 절대값을 합산한다. 좌표축에 정렬된 특성을 가지며, 개별 feature의 차이가 독립적으로 기여한다.

**L2 Distance (Euclidean)**

$$d_{L2}(x, y) = \sqrt{\sum_i (x_i - y_i)^2}$$

직선 거리를 계산한다. 회전 불변성을 가지며, 큰 차이에 더 큰 페널티를 부여한다.

| 특성 | L1 | L2 |
|------|----|----|
| 등거리선 형태 | 다이아몬드 | 원 |
| 좌표계 의존성 | 있음 | 없음 (회전 불변) |
| 적합한 경우 | feature 간 독립성이 높을 때 | 일반적인 경우 |

어떤 거리 척도가 더 좋은지는 데이터와 문제에 따라 다르므로, 실험을 통해 결정하는 것이 일반적이다.

## 2.3 한계

K-NN의 시간 복잡도는 학습 O(1), 예측 O(N)이다. 학습은 데이터 저장만 수행하지만, 예측 시에는 N개 전체 샘플과 거리를 계산해야 한다. 실제 서비스에서는 예측이 실시간으로 반복되므로, **학습은 빠르지만 예측이 느린** 이 구조는 치명적인 단점이다.

K-NN이 이미지 분류에 거의 사용되지 않는 이유를 정리하면 다음과 같다:

1. **느린 예측 시간**: 예측 시 전체 학습 데이터와 비교 필요
2. **지각적 거리 불일치**: L1/L2 거리는 인간의 시각적 유사성을 반영하지 못함. 원본 이미지를 약간 이동하거나 밝기를 변경해도 L2 거리는 크게 달라질 수 있음
3. **차원의 저주(Curse of Dimensionality)**: 고차원 공간에서 데이터가 희소해지면서 "가까운 이웃"의 의미가 퇴색됨. 공간을 균일하게 커버하려면 차원에 대해 지수적으로 많은 데이터가 필요

# 3. 하이퍼파라미터와 검증

K-NN에서 K 값이나 거리 척도는 학습 데이터로부터 자동으로 결정되지 않는다. 이처럼 **학습 전에 사람이 설정해야 하는 값**을 하이퍼파라미터(Hyperparameter)라고 한다.

## 3.1 데이터셋 분할

최적의 하이퍼파라미터를 찾기 위해서는 올바른 데이터 분할이 필수적이다.

| 데이터셋 | 용도 |
|----------|------|
| **Train** | 모델 파라미터 학습 |
| **Validation** | 하이퍼파라미터 튜닝 |
| **Test** | 최종 성능 평가 |

중요한 원칙은 **Test 세트는 개발 과정에서 절대 사용하지 않는다**는 것이다. Test 세트로 하이퍼파라미터를 튜닝하면 해당 세트에 과적합되어 실제 일반화 성능을 측정할 수 없게 된다.

## 3.2 교차 검증 (Cross-Validation)

데이터가 충분하지 않을 때는 **K-Fold Cross-Validation**을 사용할 수 있다. 학습 데이터를 K개의 폴드로 분할하고, 각 반복에서 하나의 폴드를 검증용으로 사용하며 나머지로 학습한다. K번의 실험 결과를 평균하여 성능을 추정한다.

이 방식은 모든 데이터를 학습과 검증에 활용할 수 있어 더 신뢰성 있는 성능 추정이 가능하다. 다만 학습을 K번 반복해야 하므로 계산 비용이 증가하며, 딥러닝처럼 학습 비용이 높은 경우에는 잘 사용하지 않는다.

# 4. 선형 분류기 (Linear Classifier)

K-NN의 근본적인 한계를 극복하기 위해 **파라메트릭 모델(Parametric Model)**이 등장한다. 핵심 아이디어는 학습 데이터 전체를 저장하는 대신, **데이터의 패턴을 파라미터에 압축**하는 것이다.

## 4.1 수학적 정의

선형 분류기는 다음과 같이 정의된다:

$$f(x; W, b) = Wx + b$$

- $x$: 입력 이미지를 1차원으로 펼친 벡터 (32×32×3 → 3072차원)
- $W$: 가중치 행렬 (클래스 수 × 입력 차원)
- $b$: 편향 벡터
- $f$: 출력 (각 클래스에 대한 점수)

CIFAR-10을 예로 들면, 입력은 3,072차원 벡터가 되고 10개 클래스에 대한 점수 벡터가 출력된다. 가장 높은 점수를 가진 클래스가 예측 결과가 된다.

## 4.2 해석

선형 분류기는 두 가지 관점에서 해석할 수 있다.

**템플릿 매칭 관점**: 가중치 행렬 $W$의 각 행은 해당 클래스의 **템플릿(template)**으로 볼 수 있다. 분류는 입력 이미지와 각 클래스 템플릿 간의 내적을 계산하는 것과 같으며, 내적 값이 클수록 해당 템플릿과 유사함을 의미한다.

학습된 가중치를 이미지로 시각화하면 각 클래스의 평균적인 형태를 확인할 수 있다. 그러나 클래스당 하나의 템플릿만 학습하므로 다양한 변형을 포착하지 못한다. 예를 들어 "말" 클래스에 왼쪽을 보는 말과 오른쪽을 보는 말이 섞여 있으면, 학습된 템플릿은 이 둘의 평균인 **양쪽에 머리가 있는 형태**가 되어버린다.

**기하학적 관점**: 선형 분류기는 고차원 픽셀 공간에서 **초평면(hyperplane)**으로 클래스를 분리한다. 각 클래스에 대해 하나의 선형 결정 경계가 학습되며, 이 경계를 기준으로 입력이 어느 쪽에 위치하는지에 따라 분류가 결정된다.

## 4.3 K-NN vs 선형 분류기

| 특성 | K-NN | 선형 분류기 |
|------|------|------------|
| 모델 유형 | 비파라메트릭 | 파라메트릭 |
| 학습 시간 | O(1) | 느림 (최적화 필요) |
| 예측 시간 | O(N) | O(1) |
| 저장 공간 | 전체 학습 데이터 | 파라미터만 |
| 결정 경계 | 비선형 가능 | 선형만 가능 |

선형 분류기는 학습이 완료되면 예측이 매우 빠르고, 저장 공간도 파라미터 크기에만 의존한다. 이는 실제 서비스 배포에 매우 유리한 특성이다.

## 4.4 한계

선형 분류기의 근본적인 한계는 **선형 결정 경계로는 비선형 패턴을 분리할 수 없다**는 것이다. 대표적인 예로 XOR 문제, 원형으로 분포된 데이터, 같은 클래스가 공간상 여러 영역에 분리되어 있는 경우 등이 있다.

이러한 한계를 극복하기 위해 **비선형 활성화 함수**와 **다층 구조(신경망)**가 도입되며, 이후 손실 함수(Loss Function)와 최적화(Optimization)를 통해 가중치를 학습하는 방법을 다루게 된다.

---

# References

- [Stanford CS231n Lecture 2: Image Classification](https://www.youtube.com/watch?v=OoUX-nOEjG0)
- [CS231n Course Notes](https://cs231n.github.io/)
